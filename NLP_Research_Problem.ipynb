{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecb9607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import requests\n",
    "import pytextrank\n",
    "import yake\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from rake_nltk import Rake\n",
    "import en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# pd.set_option('display.max_colwidth', 200)\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38d3cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge Graph\n",
    "def get_entities(sent):\n",
    "  ## chunk 1\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "\n",
    "    prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "    prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "    \n",
    "    #############################################################\n",
    "\n",
    "    for tok in nlp(sent):\n",
    "    ## chunk 2\n",
    "        #remove stopword\n",
    "        if (tok.text.lower() not in stop_words):\n",
    "            #remove digits\n",
    "            New_text = re.sub(\"[0-9]\",\"\",tok.text)\n",
    "            #remove tags\n",
    "            New_text = re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",New_text)\n",
    "            #remove words less than two letters\n",
    "            if(len(New_text)>2):\n",
    "        # if token is a punctuation mark then move on to the next token\n",
    "                if tok.dep_ != \"punct\":\n",
    "              # check: token is a compound word or not\n",
    "                    if tok.dep_ == \"compound\":\n",
    "                        prefix = New_text\n",
    "                # if the previous word was also a 'compound' then add the current word to it\n",
    "                        if prv_tok_dep == \"compound\":\n",
    "                            prefix = prv_tok_text + \" \"+ New_text\n",
    "\n",
    "              # check: token is a modifier or not\n",
    "                    if tok.dep_.endswith(\"mod\") == True:\n",
    "                        modifier = New_text\n",
    "                    # if the previous word was also a 'compound' then add the current word to it\n",
    "                        if prv_tok_dep == \"compound\":\n",
    "                            modifier = prv_tok_text + \" \"+ New_text\n",
    "\n",
    "              ## chunk 3\n",
    "                    if tok.dep_.find(\"subj\") == True:\n",
    "                        ent1 = modifier +\" \"+ prefix + \" \"+ New_text\n",
    "                        prefix = \"\"\n",
    "                        modifier = \"\"\n",
    "                        prv_tok_dep = \"\"\n",
    "                        prv_tok_text = \"\"      \n",
    "\n",
    "                    ## chunk 4\n",
    "                    if tok.dep_.find(\"obj\") == True:\n",
    "                        ent2 = modifier +\" \"+ prefix +\" \"+ New_text\n",
    "\n",
    "          ## chunk 5  \n",
    "          # update variables\n",
    "                    prv_tok_dep = tok.dep_\n",
    "                    prv_tok_text = New_text\n",
    "\n",
    "  #############################################################\n",
    "    return [ent2.strip(), ent1.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee72d83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rake_NLTK_Keywords(doc):\n",
    "    \n",
    "    r = Rake()\n",
    "    keywordList_Rake = []\n",
    "\n",
    "    for i in range(len(doc)):\n",
    "        r.extract_keywords_from_text(doc[i])\n",
    "        rankedList = r.get_ranked_phrases_with_scores()\n",
    "        keywordList_Rake.append([i])\n",
    "        for keyword in rankedList:\n",
    "            keyword_updated = keyword[1].split()\n",
    "            keyword_updated_string = \" \".join(keyword_updated[:3])\n",
    "            keywordList_Rake[i].append(keyword_updated_string)\n",
    "\n",
    "    keywords_Rake_new = []\n",
    "    for val in keywordList_Rake:\n",
    "        if len(val) != 1 :\n",
    "            keywords_Rake_new.append(val)\n",
    "            \n",
    "    return (keywords_Rake_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b524334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextRank_Keywords(doc):\n",
    "    \n",
    "    keywords_Textrank = []\n",
    "\n",
    "    # load a spaCy model, depending on language, scale, etc.\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    # add PyTextRank to the spaCy pipeline\n",
    "    nlp.add_pipe(\"textrank\")\n",
    "    for i in range(len(doc)):\n",
    "        document = nlp(doc[i])\n",
    "        keywords_Textrank.append([i])\n",
    "    # examine the top-ranked phrases in the document\n",
    "        for phrase in document._.phrases:\n",
    "            keywords_Textrank[i].append(phrase.text)\n",
    "\n",
    "    keywords_Textrank_new = []\n",
    "    for val in keywords_Textrank:\n",
    "        if len(val) != 1 :\n",
    "            keywords_Textrank_new.append(val)\n",
    "            \n",
    "    return (keywords_Textrank_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3717414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Yake_Keywords(doc):\n",
    "    \n",
    "    keywords_Yake = []\n",
    "\n",
    "    language = \"en\"\n",
    "    max_ngram_size = 3\n",
    "    deduplication_threshold = 0.9\n",
    "    numOfKeywords = 2\n",
    "\n",
    "    kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size,\n",
    "                                         dedupLim=deduplication_threshold,\n",
    "                                         top=numOfKeywords, features=None)\n",
    "    for i in range(len(doc)):\n",
    "        keywords = kw_extractor.extract_keywords(doc[i])\n",
    "        keywords_Yake.append([i])\n",
    "        for kw in keywords:\n",
    "            keywords_Yake[i].append(kw[0])\n",
    "\n",
    "\n",
    "    # remove none value in list of keywords\n",
    "    keywords_Yake_new = []\n",
    "    for val in keywords_Yake:\n",
    "        if len(val) != 1 :\n",
    "            keywords_Yake_new.append(val)\n",
    "            \n",
    "    return (keywords_Yake_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddc293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(entity_doc):\n",
    "\n",
    "    similarity_list = []\n",
    "\n",
    "    ## Use Title for Similarity with keywords\n",
    "    doc1 = nlp(df['title'][0])\n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(len(entity_doc)):        \n",
    "        for j in range(len(entity_doc[i])):\n",
    "            if j!=0 :\n",
    "                similarity_list.append([i])\n",
    "                doc2 = nlp(entity_doc[i][j])\n",
    "                sim = doc1.similarity(doc2)    \n",
    "                if sim != 0:\n",
    "                    similarity_list[counter].append(entity_doc[i][j])\n",
    "                    similarity_list[counter].append(sim)\n",
    "                counter = counter +1\n",
    "            \n",
    "    #remove index without keywords        \n",
    "    similarity_list_new = []\n",
    "    for val in similarity_list:\n",
    "        if len(val) != 1 :\n",
    "            similarity_list_new.append(val)\n",
    "            \n",
    "    #Sort with similarity value\n",
    "    similarity_list_new.sort(key=lambda row: (row[2]), reverse=True)\n",
    "    \n",
    "    # cosine similarity\n",
    "    return (similarity_list_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585f7569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Keywords_collectin = []\n",
    "\n",
    "for k in range(155):\n",
    "    print(k)\n",
    "    entity_pairs = []\n",
    "    Rake_ky = []\n",
    "    Textrank_ky = []\n",
    "    Yake_ky = []\n",
    "    keywords_all = []\n",
    "    similarity_KG = []\n",
    "    \n",
    "    d = pd.read_csv(\"test_dataset/Stanza Text files/%d.txt\" % k, delimiter='\\n', on_bad_lines='skip')\n",
    "    df = pd.DataFrame(data=d)\n",
    "\n",
    "    # get entity pairs\n",
    "    counter = 0\n",
    "    for i in tqdm(df['title']):\n",
    "        entity_pairs.append([counter])\n",
    "        entity_pairs[counter].extend(get_entities(i))\n",
    "\n",
    "        counter = counter + 1\n",
    "\n",
    "    # keywords extraction with Rake\n",
    "    Rake_ky = Rake_NLTK_Keywords(df['title'])\n",
    "\n",
    "    # keywords extraction with Textrank\n",
    "    Textrank_ky = TextRank_Keywords(df['title'])\n",
    "\n",
    "    # keywords extraction with Yake\n",
    "#     Yake_ky = Yake_Keywords(df['title'])\n",
    "\n",
    "    # Merge all of keywords\n",
    "    keywords_all.extend(entity_pairs[:20])\n",
    "    keywords_all.extend(Rake_ky[:20])\n",
    "    keywords_all.extend(Textrank_ky[:20])\n",
    "#     keywords_all.extend(Yake_ky[:20])\n",
    "    \n",
    "    # get similarity with title   \n",
    "    similarity_KG = get_similarity(keywords_all)\n",
    "    \n",
    "    Keywords_collectin.append([k])\n",
    "    Keywords_collectin[k].append(similarity_KG[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c2b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"test_dataset/Test_Keywords/test_collection.csv\", delimiter='\\n', header = None)\n",
    "test_set_1 = pd.DataFrame(data=d)\n",
    "\n",
    "test_set_2 = test_set_1.values.tolist()\n",
    "test_set_list = list()\n",
    "\n",
    "for item in test_set_2:\n",
    "    item = item[0]\n",
    "    item = item.replace(\"['\", \"\")\n",
    "    item = item.replace(\"']\", \"\")\n",
    "    item = item.replace(\"', '\", \",\")\n",
    "    test_set_list.append(item)\n",
    "    \n",
    "y_true =  [1] * len(test_set_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9278f438",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Compare test_set and our keywords collection ------> Determine y_pred\n",
    "Keywords_collection_list = [rec[1] for rec in Keywords_collectin]\n",
    "y_pred = list()\n",
    "\n",
    "for i, item in enumerate(Keywords_collection_list):\n",
    "    if \",\" not in test_set_list[i]:\n",
    "        if Keywords_collection_list[i].lower() == test_set_list[i].lower():\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "    elif \",\" in test_set_list[i] and len(test_set_list[i].split(\",\")) > 1:\n",
    "        if Keywords_collection_list[i].lower() in test_set_list[i].lower().split(\",\"):\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "    else:\n",
    "        print(\"Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8654424",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85a9543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "\n",
    "print(precision_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recall_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1856a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d13f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc29b64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
